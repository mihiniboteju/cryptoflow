{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29732640",
   "metadata": {},
   "source": [
    "# Phase 1: Data Cleaning & Ground Truth Generation\n",
    "\n",
    "## Objectives\n",
    "1. Load 8 years of BTC hourly OHLCV data from Binance\n",
    "2. Handle missing hours using domain-appropriate interpolation\n",
    "3. Implement Garman-Klass volatility estimator\n",
    "4. Create forward-shifted target variable for next-hour prediction\n",
    "5. Validate data quality and save cleaned dataset\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09dbd30e",
   "metadata": {},
   "source": [
    "## 1. Setup & Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fde32cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.precision', 6)\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"âœ“ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fae5eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "DATA_FILE = Path('btc_1h_data_2018_to_2025.csv')\n",
    "print(f\"Loading data from: {DATA_FILE}\")\n",
    "\n",
    "df = pd.read_csv(DATA_FILE)\n",
    "\n",
    "# Display basic info\n",
    "print(f\"\\nâœ“ Loaded {len(df):,} hourly records\")\n",
    "print(f\"âœ“ Date range: {df.iloc[0]['timestamp'] if 'timestamp' in df.columns else 'N/A'} to {df.iloc[-1]['timestamp'] if 'timestamp' in df.columns else 'N/A'}\")\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"\\nColumn names:\\n{df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7eb42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview first few rows\n",
    "print(\"First 10 rows of raw data:\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895b3f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data types and missing values summary\n",
    "print(\"Data Types:\")\n",
    "print(df.dtypes)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Missing Values Summary:\")\n",
    "missing_summary = pd.DataFrame({\n",
    "    'Missing_Count': df.isnull().sum(),\n",
    "    'Missing_Percentage': (df.isnull().sum() / len(df) * 100).round(2)\n",
    "})\n",
    "print(missing_summary[missing_summary['Missing_Count'] > 0])\n",
    "\n",
    "if missing_summary['Missing_Count'].sum() == 0:\n",
    "    print(\"\\nâœ“ No missing values detected!\")\n",
    "else:\n",
    "    print(f\"\\nâš  Total missing values: {missing_summary['Missing_Count'].sum():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea36115f",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Data Cleaning & Missing Value Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fcf185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize column names (lowercase)\n",
    "df.columns = df.columns.str.lower()\n",
    "\n",
    "# Parse timestamp if exists\n",
    "if 'timestamp' in df.columns:\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    df = df.sort_values('timestamp').reset_index(drop=True)\n",
    "    print(\"âœ“ Timestamp parsed and sorted\")\n",
    "elif 'date' in df.columns:\n",
    "    df['timestamp'] = pd.to_datetime(df['date'])\n",
    "    df = df.sort_values('timestamp').reset_index(drop=True)\n",
    "    print(\"âœ“ Date column converted to timestamp and sorted\")\n",
    "else:\n",
    "    print(\"âš  No timestamp column found - assuming data is already sorted chronologically\")\n",
    "\n",
    "print(f\"\\nData spans from {df['timestamp'].min()} to {df['timestamp'].max()}\" if 'timestamp' in df.columns else \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5203defc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for temporal gaps (missing hours)\n",
    "if 'timestamp' in df.columns:\n",
    "    # Create expected hourly range\n",
    "    expected_range = pd.date_range(\n",
    "        start=df['timestamp'].min(),\n",
    "        end=df['timestamp'].max(),\n",
    "        freq='1H'\n",
    "    )\n",
    "    \n",
    "    expected_count = len(expected_range)\n",
    "    actual_count = len(df)\n",
    "    missing_hours = expected_count - actual_count\n",
    "    \n",
    "    print(f\"Expected hourly records: {expected_count:,}\")\n",
    "    print(f\"Actual records: {actual_count:,}\")\n",
    "    print(f\"Missing hours: {missing_hours:,} ({missing_hours/expected_count*100:.2f}%)\")\n",
    "    \n",
    "    if missing_hours > 0:\n",
    "        print(\"\\nâš  Temporal gaps detected - will reindex and interpolate\")\n",
    "        \n",
    "        # Reindex to complete hourly series\n",
    "        df = df.set_index('timestamp').reindex(expected_range).rename_axis('timestamp').reset_index()\n",
    "        print(f\"âœ“ Reindexed to {len(df):,} hourly records\")\n",
    "    else:\n",
    "        print(\"\\nâœ“ No temporal gaps - data is complete!\")\n",
    "else:\n",
    "    print(\"âš  Cannot check for temporal gaps without timestamp column\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bc1799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flag interpolated rows (before interpolation)\n",
    "df['is_interpolated'] = df[['open', 'high', 'low', 'close']].isnull().any(axis=1).astype(int)\n",
    "interpolated_count = df['is_interpolated'].sum()\n",
    "print(f\"Rows requiring interpolation: {interpolated_count:,} ({interpolated_count/len(df)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab30b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values by type\n",
    "print(\"Applying interpolation strategy...\\n\")\n",
    "\n",
    "# Price columns: Linear interpolation (continuous process assumption)\n",
    "price_cols = ['open', 'high', 'low', 'close']\n",
    "for col in price_cols:\n",
    "    if col in df.columns:\n",
    "        before_nulls = df[col].isnull().sum()\n",
    "        df[col] = df[col].interpolate(method='linear', limit_direction='both')\n",
    "        after_nulls = df[col].isnull().sum()\n",
    "        print(f\"  â€¢ {col.upper()}: Interpolated {before_nulls - after_nulls} missing values\")\n",
    "\n",
    "# Volume & trade count: Zero-filling (no trading activity assumption)\n",
    "volume_cols = ['volume', 'trade_count', 'taker_buy_volume', 'taker_buy_quote_volume']\n",
    "for col in volume_cols:\n",
    "    if col in df.columns:\n",
    "        before_nulls = df[col].isnull().sum()\n",
    "        df[col] = df[col].fillna(0)\n",
    "        after_nulls = df[col].isnull().sum()\n",
    "        if before_nulls > 0:\n",
    "            print(f\"  â€¢ {col.upper()}: Zero-filled {before_nulls - after_nulls} missing values\")\n",
    "\n",
    "print(\"\\nâœ“ Interpolation complete\")\n",
    "print(f\"âœ“ Remaining nulls: {df.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66a6560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate price consistency (High >= Low, High >= Open/Close, Low <= Open/Close)\n",
    "print(\"Validating OHLC price consistency...\\n\")\n",
    "\n",
    "issues = []\n",
    "\n",
    "# Check 1: High >= Low\n",
    "invalid_hl = (df['high'] < df['low']).sum()\n",
    "if invalid_hl > 0:\n",
    "    issues.append(f\"âš  {invalid_hl} rows where High < Low\")\n",
    "else:\n",
    "    print(\"âœ“ All High >= Low\")\n",
    "\n",
    "# Check 2: High >= Open and Close\n",
    "invalid_ho = (df['high'] < df['open']).sum()\n",
    "invalid_hc = (df['high'] < df['close']).sum()\n",
    "if invalid_ho > 0 or invalid_hc > 0:\n",
    "    issues.append(f\"âš  High < Open: {invalid_ho}, High < Close: {invalid_hc}\")\n",
    "else:\n",
    "    print(\"âœ“ All High >= Open and Close\")\n",
    "\n",
    "# Check 3: Low <= Open and Close\n",
    "invalid_lo = (df['low'] > df['open']).sum()\n",
    "invalid_lc = (df['low'] > df['close']).sum()\n",
    "if invalid_lo > 0 or invalid_lc > 0:\n",
    "    issues.append(f\"âš  Low > Open: {invalid_lo}, Low > Close: {invalid_lc}\")\n",
    "else:\n",
    "    print(\"âœ“ All Low <= Open and Close\")\n",
    "\n",
    "# Check 4: No zero or negative prices\n",
    "invalid_prices = ((df[price_cols] <= 0).sum(axis=1) > 0).sum()\n",
    "if invalid_prices > 0:\n",
    "    issues.append(f\"âš  {invalid_prices} rows with zero or negative prices\")\n",
    "else:\n",
    "    print(\"âœ“ No zero or negative prices\")\n",
    "\n",
    "if issues:\n",
    "    print(\"\\n\" + \"\\n\".join(issues))\n",
    "    print(\"\\nNote: These anomalies may be due to flash crashes or data errors.\")\n",
    "    print(\"Consider manual inspection or outlier removal if count is significant.\")\n",
    "else:\n",
    "    print(\"\\nâœ“ All OHLC validations passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2967b01",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Garman-Klass Volatility Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1cf9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def garman_klass_volatility(open_price, high_price, low_price, close_price):\n",
    "    \"\"\"\n",
    "    Compute Garman-Klass (1980) volatility estimator.\n",
    "    \n",
    "    Formula:\n",
    "    GK = sqrt(0.5 * [log(H/L)]^2 - (2*log(2) - 1) * [log(C/O)]^2)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    open_price : float\n",
    "        Opening price of interval\n",
    "    high_price : float\n",
    "        Highest price during interval\n",
    "    low_price : float\n",
    "        Lowest price during interval\n",
    "    close_price : float\n",
    "        Closing price of interval\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    float : Garman-Klass volatility estimate (returns NaN for invalid prices)\n",
    "    \n",
    "    Reference:\n",
    "    ----------\n",
    "    Garman, M. B., & Klass, M. J. (1980). On the estimation of security price \n",
    "    volatilities from historical data. Journal of Business, 53(1), 67-78.\n",
    "    \"\"\"\n",
    "    # Validate inputs\n",
    "    if any(pd.isna([open_price, high_price, low_price, close_price])):\n",
    "        return np.nan\n",
    "    \n",
    "    if any(p <= 0 for p in [open_price, high_price, low_price, close_price]):\n",
    "        return np.nan\n",
    "    \n",
    "    # Compute log ratios\n",
    "    hl_ratio = np.log(high_price / low_price)\n",
    "    co_ratio = np.log(close_price / open_price)\n",
    "    \n",
    "    # Garman-Klass formula\n",
    "    gk = np.sqrt(\n",
    "        0.5 * hl_ratio**2 - (2 * np.log(2) - 1) * co_ratio**2\n",
    "    )\n",
    "    \n",
    "    return gk\n",
    "\n",
    "print(\"âœ“ Garman-Klass function defined\")\n",
    "print(\"\\nFunction signature: garman_klass_volatility(open, high, low, close)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8832012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Garman-Klass formula to entire dataset\n",
    "print(\"Computing GK volatility for all hourly intervals...\\n\")\n",
    "\n",
    "df['gk_volatility'] = df.apply(\n",
    "    lambda row: garman_klass_volatility(\n",
    "        row['open'], \n",
    "        row['high'], \n",
    "        row['low'], \n",
    "        row['close']\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Check for NaN values\n",
    "gk_nulls = df['gk_volatility'].isnull().sum()\n",
    "print(f\"âœ“ GK volatility computed for {len(df) - gk_nulls:,} intervals\")\n",
    "if gk_nulls > 0:\n",
    "    print(f\"âš  {gk_nulls} NaN values (likely due to invalid prices)\")\n",
    "    print(\"  These will be handled in feature engineering phase\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907c2647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics for GK volatility\n",
    "print(\"Garman-Klass Volatility Statistics:\")\n",
    "print(\"=\" * 60)\n",
    "print(df['gk_volatility'].describe())\n",
    "\n",
    "# Identify potential outliers (>3 std deviations)\n",
    "gk_mean = df['gk_volatility'].mean()\n",
    "gk_std = df['gk_volatility'].std()\n",
    "outlier_threshold = gk_mean + 3 * gk_std\n",
    "outliers = (df['gk_volatility'] > outlier_threshold).sum()\n",
    "\n",
    "print(f\"\\nPotential outliers (>3Ïƒ): {outliers} ({outliers/len(df)*100:.3f}%)\")\n",
    "print(f\"Outlier threshold: {outlier_threshold:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237ac495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize GK volatility distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogram\n",
    "axes[0].hist(df['gk_volatility'].dropna(), bins=100, edgecolor='black', alpha=0.7)\n",
    "axes[0].axvline(gk_mean, color='red', linestyle='--', linewidth=2, label=f'Mean: {gk_mean:.6f}')\n",
    "axes[0].axvline(outlier_threshold, color='orange', linestyle='--', linewidth=2, label=f'3Ïƒ: {outlier_threshold:.6f}')\n",
    "axes[0].set_xlabel('GK Volatility', fontsize=12)\n",
    "axes[0].set_ylabel('Frequency', fontsize=12)\n",
    "axes[0].set_title('Distribution of Garman-Klass Volatility', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Time series (downsampled for visibility if >10k points)\n",
    "if 'timestamp' in df.columns:\n",
    "    plot_df = df[['timestamp', 'gk_volatility']].dropna()\n",
    "    if len(plot_df) > 10000:\n",
    "        plot_df = plot_df.iloc[::10]  # Plot every 10th point\n",
    "    axes[1].plot(plot_df['timestamp'], plot_df['gk_volatility'], linewidth=0.5, alpha=0.7)\n",
    "    axes[1].axhline(gk_mean, color='red', linestyle='--', linewidth=1, label='Mean')\n",
    "    axes[1].set_xlabel('Time', fontsize=12)\n",
    "    axes[1].set_ylabel('GK Volatility', fontsize=12)\n",
    "    axes[1].set_title('GK Volatility Over Time', fontsize=14, fontweight='bold')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(alpha=0.3)\n",
    "else:\n",
    "    axes[1].text(0.5, 0.5, 'Timestamp not available', ha='center', va='center', fontsize=14)\n",
    "    axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ Volatility visualizations generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b9351f",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Target Variable Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24b6495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create target: next hour's GK volatility\n",
    "print(\"Creating target variable: next-hour GK volatility\\n\")\n",
    "\n",
    "# Shift GK volatility backward by 1 (shift=-1 means look ahead)\n",
    "df['target_gk_next_hour'] = df['gk_volatility'].shift(-1)\n",
    "\n",
    "# Count valid targets\n",
    "valid_targets = df['target_gk_next_hour'].notna().sum()\n",
    "print(f\"âœ“ Target variable created: {valid_targets:,} valid samples\")\n",
    "print(f\"  (Last row has no future target, as expected)\")\n",
    "\n",
    "# Remove last row (no future target)\n",
    "df_original_len = len(df)\n",
    "df = df[:-1].copy()\n",
    "print(f\"\\nâœ“ Dropped last row: {df_original_len:,} â†’ {len(df):,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0491633d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify no data leakage: current GK should differ from target\n",
    "print(\"Data Leakage Check:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Correlation between current and next-hour volatility\n",
    "correlation = df[['gk_volatility', 'target_gk_next_hour']].corr().iloc[0, 1]\n",
    "print(f\"Correlation(GK_current, GK_next_hour): {correlation:.4f}\")\n",
    "\n",
    "if correlation > 0.95:\n",
    "    print(\"âš  Very high correlation - may indicate data leakage or trivial prediction\")\n",
    "elif correlation > 0.5:\n",
    "    print(\"âœ“ Moderate correlation - volatility clustering detected (expected in finance)\")\n",
    "else:\n",
    "    print(\"âœ“ Low correlation - prediction task is non-trivial\")\n",
    "\n",
    "# Check if any rows are identical\n",
    "identical_rows = (df['gk_volatility'] == df['target_gk_next_hour']).sum()\n",
    "print(f\"\\nRows where current GK == next GK: {identical_rows} ({identical_rows/len(df)*100:.2f}%)\")\n",
    "print(\"  (Some matches are expected in stable market conditions)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410892c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize target distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Target distribution\n",
    "axes[0].hist(df['target_gk_next_hour'].dropna(), bins=100, edgecolor='black', alpha=0.7, color='green')\n",
    "axes[0].set_xlabel('Target: Next-Hour GK Volatility', fontsize=12)\n",
    "axes[0].set_ylabel('Frequency', fontsize=12)\n",
    "axes[0].set_title('Distribution of Target Variable', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Scatter: current vs next-hour volatility\n",
    "sample_size = min(5000, len(df))  # Downsample for visibility\n",
    "sample_df = df.sample(n=sample_size, random_state=42)\n",
    "axes[1].scatter(sample_df['gk_volatility'], sample_df['target_gk_next_hour'], \n",
    "                alpha=0.3, s=10, color='blue')\n",
    "axes[1].plot([df['gk_volatility'].min(), df['gk_volatility'].max()],\n",
    "             [df['gk_volatility'].min(), df['gk_volatility'].max()],\n",
    "             'r--', linewidth=2, label='y=x (perfect correlation)')\n",
    "axes[1].set_xlabel('Current GK Volatility', fontsize=12)\n",
    "axes[1].set_ylabel('Next-Hour GK Volatility', fontsize=12)\n",
    "axes[1].set_title(f'Current vs Next-Hour Volatility (n={sample_size:,})', fontsize=14, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ Target variable visualizations generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f4992c",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Final Data Quality Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c4e319",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PHASE 1 COMPLETION REPORT: DATA CLEANING & GROUND TRUTH\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nðŸ“Š Dataset Statistics:\")\n",
    "print(f\"  â€¢ Total hourly records: {len(df):,}\")\n",
    "if 'timestamp' in df.columns:\n",
    "    print(f\"  â€¢ Time span: {df['timestamp'].min()} to {df['timestamp'].max()}\")\n",
    "    total_days = (df['timestamp'].max() - df['timestamp'].min()).days\n",
    "    print(f\"  â€¢ Duration: {total_days:,} days ({total_days/365.25:.1f} years)\")\n",
    "\n",
    "print(f\"\\nðŸ”§ Data Cleaning:\")\n",
    "print(f\"  â€¢ Interpolated rows: {df['is_interpolated'].sum():,} ({df['is_interpolated'].mean()*100:.2f}%)\")\n",
    "print(f\"  â€¢ Remaining nulls: {df.isnull().sum().sum()}\")\n",
    "\n",
    "print(f\"\\nðŸ“ˆ Garman-Klass Volatility:\")\n",
    "print(f\"  â€¢ Valid GK calculations: {df['gk_volatility'].notna().sum():,}\")\n",
    "print(f\"  â€¢ Mean volatility: {df['gk_volatility'].mean():.6f}\")\n",
    "print(f\"  â€¢ Std volatility: {df['gk_volatility'].std():.6f}\")\n",
    "print(f\"  â€¢ Min volatility: {df['gk_volatility'].min():.6f}\")\n",
    "print(f\"  â€¢ Max volatility: {df['gk_volatility'].max():.6f}\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Target Variable:\")\n",
    "print(f\"  â€¢ Valid targets: {df['target_gk_next_hour'].notna().sum():,}\")\n",
    "print(f\"  â€¢ Target mean: {df['target_gk_next_hour'].mean():.6f}\")\n",
    "print(f\"  â€¢ Autocorrelation: {correlation:.4f}\")\n",
    "\n",
    "print(f\"\\nðŸ’¾ Output Columns: {len(df.columns)}\")\n",
    "print(f\"  {df.columns.tolist()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ“ PHASE 1 COMPLETE - Ready for Feature Engineering\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb43d43",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Save Cleaned Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db613fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV\n",
    "OUTPUT_FILE = Path('cleaned_data_with_gk_target.csv')\n",
    "df.to_csv(OUTPUT_FILE, index=False)\n",
    "\n",
    "print(f\"âœ“ Cleaned dataset saved to: {OUTPUT_FILE}\")\n",
    "print(f\"âœ“ File size: {OUTPUT_FILE.stat().st_size / (1024**2):.2f} MB\")\n",
    "print(f\"\\nNext step: Open 2_feature_engineering.ipynb to create 50+ candidate features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f5244d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick preview of saved file structure\n",
    "print(\"\\nFinal dataset preview (first 5 rows):\")\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
